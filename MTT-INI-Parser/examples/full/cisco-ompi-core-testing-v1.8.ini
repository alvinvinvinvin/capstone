#======================================================================
# Cisco configuration
#======================================================================

[MTT]
# No proxies needed on this cluster!
description = Cisco community testing cluster

min_disk_free = 1%
min_disk_free_wait = 30

funclet_files = &dirname("@INI_NAME@")/Cisco.pm

trial = 0

#======================================================================
# MPI get phase
#======================================================================

[MPI get: ompi-nightly-v1.8]
mpi_details = OMPI v1.8

module = OMPI_Snapshot
ompi_snapshot_url = http://www.open-mpi.org/nightly/v1.8
ompi_snapshot_version_file = &getenv("HOME")/mtt-versions/community/v1.8&getenv("MTT_VERSION_FILE_SUFFIX").txt

#======================================================================
# Install MPI phase
#======================================================================

# This section is not used directly; it is included in others.
[Configure MPI install]
mpi_get = all
save_stdout_on_success = 1
merge_stdout_stderr = 0

module = OMPI
ompi_vpath_mode = none
ompi_make_all_arguments = -j 32
ompi_make_check = 1

#----------------------------------------------------------------------

# This section is not used directly; it is included in others.
[Configure GNU MPI install]
include_section = Configure MPI install

env_module = cisco/gcc

ompi_compiler_name = gnu
ompi_compiler_version = &get_gcc_version()

#----------------------------------------------------------------------
#- Test different compilers, and a few different compiler options
#----------------------------------------------------------------------

[MPI install: GNU-standard]
include_section = Configure GNU MPI install

ompi_configure_arguments = "CFLAGS=-g -pipe" --enable-picky --enable-debug --enable-mpirun-prefix-by-default --disable-dlopen

#----------------------------------------------------------------------

[MPI install: GNU-threaded]
include_section = Configure GNU MPI install

ompi_configure_arguments = "CFLAGS=-g -pipe" --enable-picky --enable-debug --enable-mpirun-prefix-by-default --with-threads --disable-dlopen

#----------------------------------------------------------------------

# MPI_WAITALL is currently causing a hang with
# THREAD_MULTIPLE-initialized jobs; see
# https://svn.open-mpi.org/trac/ompi/ticket/3611.  It is causing
# massive/long hangs; skipping this section until #3611 is fixed.
[SKIP MPI install: GNU-threaded-thread-multiple]
include_section = Configure GNU MPI install

ompi_configure_arguments = "CFLAGS=-g -pipe" --enable-picky --enable-debug --enable-mpirun-prefix-by-default --with-threads --disable-dlopen --enable-mpi-thread-multiple

#----------------------------------------------------------------------

[MPI install: clang-3.5.1]
include_section = Configure MPI install

env_module = cisco/clang-compilers/3.5.1

ompi_compiler_name = clang
ompi_compiler_version = &get_clang_version()
ompi_configure_arguments = CC=clang CXX=clang++ --disable-mpi-fortran "CFLAGS=-g -pipe" --enable-picky --enable-debug --enable-mpirun-prefix-by-default --disable-dlopen

#----------------------------------------------------------------------

[MPI install: clang-3.6]
include_section = Configure MPI install

env_module = cisco/clang-compilers/3.6

ompi_compiler_name = clang
ompi_compiler_version = &get_clang_version()
ompi_configure_arguments = CC=clang CXX=clang++ --disable-mpi-fortran "CFLAGS=-g -pipe" --enable-picky --enable-debug --enable-mpirun-prefix-by-default --disable-dlopen

#----------------------------------------------------------------------

[MPI install: Intel-12.1]
include_section = Configure MPI install

env_module = cisco/intel-compilers/2011-12.1.4-10.319

# Force the use of MPI Fortran, just to be sure
ompi_compiler_name = intel
ompi_compiler_version = &get_icc_version()
ompi_configure_arguments = CC=icc CXX=icpc FC=ifort "CFLAGS=-g -wd188" --enable-picky --enable-debug --enable-mpirun-prefix-by-default --disable-dlopen --enable-mpi-fortran

#----------------------------------------------------------------------

[MPI install: Intel-14.0]
include_section = Configure MPI install

env_module = cisco/intel-compilers/2013-14.0.3.174

# Force the use of MPI Fortran, just to be sure
ompi_compiler_name = intel
ompi_compiler_version = &get_icc_version()
ompi_configure_arguments = CC=icc CXX=icpc FC=ifort "CFLAGS=-g -wd188" --enable-picky --enable-debug --enable-mpirun-prefix-by-default --disable-dlopen --enable-mpi-fortran

#----------------------------------------------------------------------

[MPI install: Intel-15.0]
include_section = Configure MPI install

env_module = cisco/intel-compilers/2015-15.0.2.164

# Force the use of MPI Fortran, just to be sure
ompi_compiler_name = intel
ompi_compiler_version = &get_icc_version()
ompi_configure_arguments = CC=icc CXX=icpc FC=ifort "CFLAGS=-g -wd188" --enable-picky --enable-debug --enable-mpirun-prefix-by-default --disable-dlopen --enable-mpi-fortran

#----------------------------------------------------------------------
#- Test different OMPI configure options and other "smaller" tests
#----------------------------------------------------------------------

# This section is not used directly; it is included in others.
[Configure limited MPI install]
include_section = Configure GNU MPI install

mpi_details = OMPI v1.8 limited

#----------------------------------------------------------------------

[MPI install: GNU-autogen]
include_section = Configure limited MPI install

env_module = cisco/autotools/ac269-am1131-lt242

ompi_autogen = 1
ompi_configure_arguments = "CFLAGS=-g -pipe" --enable-picky --enable-debug --enable-mpirun-prefix-by-default --disable-dlopen

#----------------------------------------------------------------------

[MPI install: GNU-devel-headers]
include_section = Configure limited MPI install

ompi_configure_arguments = "CFLAGS=-g -pipe" --enable-picky --enable-debug --enable-mpirun-prefix-by-default --disable-dlopen --with-devel-headers

#----------------------------------------------------------------------

[MPI install: GNU-static]
include_section = Configure limited MPI install

ompi_configure_arguments = "CFLAGS=-g -pipe" --enable-picky --enable-debug --enable-mpirun-prefix-by-default --disable-shared --enable-static --disable-dlopen

#----------------------------------------------------------------------

[MPI install: GNU-static-no-romio]
include_section = Configure limited MPI install

ompi_configure_arguments = "CFLAGS=-g -pipe" --enable-picky --enable-debug --enable-mpirun-prefix-by-default --disable-shared --enable-static --disable-io-romio --disable-dlopen

#----------------------------------------------------------------------

[MPI install: GNU-32bit]
include_section = Configure limited MPI install

ompi_configure_arguments = <<EOT
"CFLAGS=-g -pipe -m32" CXXFLAGS=-m32 FFLAGS=-m32 FCFLAGS=-m32
--with-wrapper-cflags=-m32 --with-wrapper-cxxflags=-m32
--with-wrapper-fcflags=-m32
--enable-mpirun-prefix-by-default --disable-dlopen
EOT

#----------------------------------------------------------------------

[MPI install: GNU-No-cxx]
include_section = Configure limited MPI install

ompi_configure_arguments = "CFLAGS=-g -pipe" --enable-picky --enable-debug --enable-mpirun-prefix-by-default --disable-dlopen --disable-mpi-cxx

#----------------------------------------------------------------------

[MPI install: GNU-No-fortran]
include_section = Configure limited MPI install

ompi_configure_arguments = "CFLAGS=-g -pipe" --enable-picky --enable-debug --enable-mpirun-prefix-by-default --disable-dlopen --disable-mpi-fortran

#----------------------------------------------------------------------

[MPI install: GNU-No-vt]
include_section = Configure limited MPI install

ompi_configure_arguments = "CFLAGS=-g -pipe" --enable-picky --enable-debug --enable-mpirun-prefix-by-default --disable-dlopen --disable-vt

#----------------------------------------------------------------------

[MPI install: GNU-No-hwloc]
include_section = Configure limited MPI install

ompi_configure_arguments = "CFLAGS=-g -pipe" --enable-picky --enable-debug --enable-mpirun-prefix-by-default --disable-dlopen --without-hwloc

#----------------------------------------------------------------------

[MPI install: GNU-hwloc-external]
include_section = Configure limited MPI install

# Need to add hwloc's lib path to LD_LIBRARY_PATH (particularly for
# the test builds)
prepend_path = LD_LIBRARY_PATH /home/mpiteam/hwloc-1.7.2/install/lib

ompi_configure_arguments = "CFLAGS=-g -pipe" --enable-picky --enable-debug --enable-mpirun-prefix-by-default --disable-dlopen --with-hwloc=/home/mpiteam/hwloc-1.7.2/install --enable-mpi-cxx

#----------------------------------------------------------------------

# We use --disable-dlopen on every other Install section just to cut
# down on NFS traffic and improve the cluster's overall MTT
# throughput.  But here we specifically test with dlopen enabled.
[MPI install: GNU-Dlopen]
include_section = Configure limited MPI install

ompi_configure_arguments = "CFLAGS=-g -pipe" --enable-picky --enable-debug --enable-mpirun-prefix-by-default

#----------------------------------------------------------------------

[MPI install: GNU-Peruse]
include_section = Configure limited MPI install

ompi_configure_arguments = "CFLAGS=-g -pipe" --enable-picky --enable-debug --enable-mpirun-prefix-by-default --disable-dlopen --enable-peruse

#----------------------------------------------------------------------

[MPI install: GNU-No-MPI-param-check]
include_section = Configure limited MPI install

ompi_configure_arguments = "CFLAGS=-g -pipe" --enable-picky --enable-debug --enable-mpirun-prefix-by-default --disable-dlopen --without-mpi-param-check

#----------------------------------------------------------------------

[MPI install: GNU-Memchecker]
include_section = Configure limited MPI install

ompi_configure_arguments = "CFLAGS=-g -pipe" --enable-picky --enable-debug --enable-mpirun-prefix-by-default --disable-dlopen --enable-memchecker --with-valgrind=/cm/shared/apps/valgrind/current

#----------------------------------------------------------------------

#JMS Heterogeneous is busted
[SKIP MPI install: GNU-Heterogeneous]
include_section = Configure GNU MPI install

ompi_configure_arguments = "CFLAGS=-g -pipe" --enable-picky --enable-debug --enable-mpirun-prefix-by-default --disable-dlopen --enable-heterogeneous

#----------------------------------------------------------------------

[MPI install: GNU-CXX-Exceptions]
include_section = Configure limited MPI install

ompi_configure_arguments = "CFLAGS=-g -pipe" --enable-picky --enable-debug --enable-mpirun-prefix-by-default --disable-dlopen --enable-cxx-exceptions

#----------------------------------------------------------------------

[MPI install: GNU-No-mpi-io]
include_section = Configure limited MPI install

ompi_configure_arguments = "CFLAGS=-g -pipe" --enable-picky --enable-debug --enable-mpirun-prefix-by-default --disable-dlopen --disable-mpi-io

#----------------------------------------------------------------------

[MPI install: GNU-No-memory-manager]
include_section = Configure GNU MPI install

ompi_configure_arguments = "CFLAGS=-g -pipe" --enable-picky --enable-debug --enable-mpirun-prefix-by-default --disable-dlopen --without-memory-manager

#----------------------------------------------------------------------

# These builds broke (as expected) when ORTE switched to a state
# machine.  Will re-enable when the builds are working again.
[SKIP MPI install: GNU-FT]
include_section = Configure limited MPI install

ompi_configure_arguments = "CFLAGS=-g -pipe" --enable-picky --enable-debug --enable-mpirun-prefix-by-default --disable-dlopen --with-ft=cr

#----------------------------------------------------------------------

[MPI install: GNU-dst-checksum]
include_section = Configure limited MPI install

ompi_configure_arguments = "CFLAGS=-g -pipe" --enable-picky --enable-debug --enable-mpirun-prefix-by-default --disable-dlopen --with-dst-checksum

#----------------------------------------------------------------------

[MPI install: GNU-disable-weak-symbols]
include_section = Configure limited MPI install

ompi_configure_arguments = "CFLAGS=-g -pipe" --enable-picky --enable-debug --enable-mpirun-prefix-by-default --disable-dlopen --disable-weak-symbols

#----------------------------------------------------------------------

[MPI install: GNU-ipv6]
include_section = Configure limited MPI install

ompi_configure_arguments = "CFLAGS=-g -pipe" --enable-picky --enable-debug --enable-mpirun-prefix-by-default --disable-dlopen --enable-ipv6

#----------------------------------------------------------------------

[MPI install: GNU-no-visibility]
include_section = Configure limited MPI install

ompi_configure_arguments = "CFLAGS=-g -pipe" --enable-picky --enable-debug --enable-mpirun-prefix-by-default --disable-dlopen --disable-visibility

#----------------------------------------------------------------------

[MPI install: GNU-mpi-extensions]
include_section = Configure limited MPI install

ompi_configure_arguments = "CFLAGS=-g -pipe" --enable-picky --enable-debug --enable-mpirun-prefix-by-default --disable-dlopen --enable-mpi-ext=all

#----------------------------------------------------------------------

# Per https://svn.open-mpi.org/trac/ompi/ticket/3081, the builtin
# atomics are failing
[SKIP MPI install: GNU-builtin-atomics]
include_section = Configure limited MPI install

ompi_configure_arguments = "CFLAGS=-g -pipe" --enable-picky --enable-debug --enable-mpirun-prefix-by-default --disable-dlopen --enable-builtin-atomics

#----------------------------------------------------------------------

[MPI install: GNU-sparse-groups]
include_section = Configure limited MPI install

ompi_configure_arguments = "CFLAGS=-g -pipe" --enable-picky --enable-debug --enable-mpirun-prefix-by-default --disable-dlopen --enable-sparse-groups

#----------------------------------------------------------------------

[MPI install: GNU-no-branch-prediction]
include_section = Configure limited MPI install

ompi_configure_arguments = "CFLAGS=-g -pipe" --enable-picky --enable-debug --enable-mpirun-prefix-by-default --disable-dlopen --disable-branch-probabilities

#----------------------------------------------------------------------

[MPI install: GNU-no-pty]
include_section = Configure limited MPI install

ompi_configure_arguments = "CFLAGS=-g -pipe" --enable-picky --enable-debug --enable-mpirun-prefix-by-default --disable-dlopen --disable-pty-support

#----------------------------------------------------------------------

[MPI install: GNU-Java]
include_section = Configure limited MPI install

ompi_configure_arguments = "CFLAGS=-g -pipe" --enable-picky --enable-debug --enable-mpirun-prefix-by-default --disable-dlopen --enable-mpi-java

#----------------------------------------------------------------------

# Use ifort with 8 byte INTEGERs and gcc with 4 byte int's
[SKIP MPI install: Int-vs-INTEGER]
include_section = Configure limited MPI install

ompi_configure_arguments = FC=ifort "FFLAGS=-i8 -r16" "FCFLAGS=-i8 -r16" "CFLAGS=-g -pipe" --enable-picky --enable-debug --enable-mpirun-prefix-by-default --disable-dlopen --disable-mpi-io --with-wrapper-fflags="-i8 -r16" --with-wrapper-fcflags="-i8 -r16"

#======================================================================
# mpirun details
#======================================================================

[MPI Details: OMPI v1.8]

# Enable the "oversubscribe" flag so that the dynamics / spawn tests
# don't fail
exec = mpirun @orte_debug@ --oversubscribe -np &test_np() --mca orte_startup_timeout 10000 @mca@ &test_executable() &test_argv()

parameters = &MPI::OMPI::find_mpirun_params(&test_command_line(), \
                                            &test_executable())
network = &MPI::OMPI::find_network(&test_command_line(), &test_executable())

# Try to find out why I'm getting so many hangs...
#orte_debug = --mca plm rsh --mca ess_base_verbose 5 --mca errmgr_base_verbose 5 --mca plm_base_verbose 5 --mca state_base_verbose 5 --mca odls_base_verbose 5 --leave-session-attached
# JMS Add "--mca plm rsh" because something seems to be wrong with the
# slurm launcher :-(
#orte_debug = --mca plm rsh --mca orte_launch_agent &Cisco::orte_launch_agent()
orte_debug =

# Intentinally specify the BTLs here because the Dell servers we are
# have a misconfigured IB network, so we want to avoid spurious
# failures from the openib BTL having a half-baked hardware/software
# setup.  Similarly, skip the usnic BTL so that it doesn't complain
# that no usnic interfaces were found and is going to use a
# lower-performance network, yadda yadda yadda.
# JMS Sep 2014: coll ml is borked.  Just eliminate it from testing.
mca = --mca coll ^ml &enumerate( \
        "--mca btl sm,tcp,self", \
        "--mca btl vader,tcp,self", \
        "--mca btl tcp,self --mca mpi_leave_pinned 1", \
        "--mca btl tcp,self --mca mpi_leave_pinned_pipeline 1")

# Remove coll_ml_priority for the time being; it's causing massive
# timeouts
#        "--mca btl sm,tcp,self --mca coll_ml_priority 90")

#---------

# It is important that the after_all_exec step is a single
# command/line so that MTT will launch it directly (instead of via a
# temporary script).  This is because the "srun" command is
# (intentionally) difficult to kill in some cases.  See
# https://svn.open-mpi.org/trac/mtt/changeset/657 for details.

after_all_exec = &if(&ne("", &getenv("SLURM_NNODES")), "srun -N " . &getenv("SLURM_NNODES")) /home/mpiteam/git/ompi-tests/cisco/mtt/after_each_exec.pl
after_all_exec_timeout = 30

#----------------------------------------------------------------------

[MPI Details: OMPI v1.8 limited]

exec = mpirun @orte_debug@ --oversubscribe -np &test_np() --mca orte_startup_timeout 10000 @mca@ &test_executable() &test_argv()

parameters = &MPI::OMPI::find_mpirun_params(&test_command_line(), \
                                            &test_executable())
network = &MPI::OMPI::find_network(&test_command_line(), &test_executable())

# Try to find out why I'm getting so many hangs...
#orte_debug = --mca plm rsh --mca ess_base_verbose 5 --mca errmgr_base_verbose 5 --mca plm_base_verbose 5 --mca state_base_verbose 5 --mca odls_base_verbose 5 --leave-session-attached
# JMS Add "--mca plm rsh" because something seems to be wrong with the
# slurm launcher :-(
#orte_debug = --mca plm rsh --mca orte_launch_agent &Cisco::orte_launch_agent()
orte_debug =

# Only specify a few possibilities here; we are intentionally only
# running a few possibilities here.
# JMS Sep 2014: coll ml is borked.  Just eliminate it from testing.
mca = --mca coll ^ml &enumerate( \
        "--mca btl sm,tcp,self")

# Remove coll_my_priority for the time being; it's causing massive
# timeouts
#        "--mca btl sm,tcp,self --mca coll_ml_priority 90")

#---------

# It is important that the after_all_exec step is a single
# command/line so that MTT will launch it directly (instead of via a
# temporary script).  This is because the "srun" command is
# (intentionally) difficult to kill in some cases.  See
# https://svn.open-mpi.org/trac/mtt/changeset/657 for details.

after_all_exec = &if(&ne("", &getenv("SLURM_NNODES")), "srun -N " . &getenv("SLURM_NNODES")) /home/mpiteam/git/ompi-tests/cisco/mtt/after_each_exec.pl
after_all_exec_timeout = 30

#======================================================================
# Test get phase
#======================================================================

[Test get: trivial]
module = Trivial

#----------------------------------------------------------------------

[Test get: intel]
module = SCM
scm_module = Git
scm_url = /home/mpiteam/mirror-git/ompi-tests
scm_subdir = intel_tests

#----------------------------------------------------------------------

[Test get: ibm]
module = SCM
scm_module = Git
scm_url = /home/mpiteam/mirror-git/ompi-tests
scm_subdir = ibm
scm_post_copy = <<EOT
./autogen.sh
EOT

#----------------------------------------------------------------------

[Test get: onesided]
module = SCM
scm_module = Git
scm_url = /home/mpiteam/mirror-git/ompi-tests
scm_subdir = onesided
scm_post_copy = <<EOT
./autogen.sh
EOT

#----------------------------------------------------------------------

[Test get: mpicxx]
module = SCM
scm_module = Git
scm_url = /home/mpiteam/mirror-git/ompi-tests
scm_subdir = cxx-test-suite
scm_post_copy = <<EOT
./autogen.sh
EOT

#----------------------------------------------------------------------

[Test get: imb]
module = SCM
scm_module = Git
scm_url = /home/mpiteam/mirror-git/ompi-tests
scm_subdir = imb

#----------------------------------------------------------------------

[Test get: netpipe]
module = SCM
scm_module = Git
scm_url = /home/mpiteam/mirror-git/ompi-tests
scm_subdir = NetPIPE-3.7.1

#----------------------------------------------------------------------

[Test get: mpi-test-suite]
module = SCM
scm_module = Git
scm_url = /home/mpiteam/mirror-git/ompi-tests
scm_subdir = mpi_test_suite
scm_post_copy = <<EOT
./autogen.sh
EOT

#----------------------------------------------------------------------

# JMS We don't have a good java test suite right now; the one
# in SVN was the wrong/outdated one (!)
[SKIP Test get: java]
module = SCM
scm_module = Git
scm_url = /home/mpiteam/mirror-git/ompi-tests
scm_subdir = java-test-suite
scm_post_copy = <<EOT
./autogen.sh
EOT

#======================================================================
# Test build phase
#======================================================================

[Test build: trivial]
test_get = trivial
save_stdout_on_success = 1
merge_stdout_stderr = 1

module = Trivial

#----------------------------------------------------------------------

[Test build: intel]
test_get = intel
save_stdout_on_success = 1
merge_stdout_stderr = 1

module = Intel_OMPI_Tests
intel_ompi_tests_make_arguments = -j 32
intel_ompi_tests_buildfile = all_tests_no_perf

#----------------------------------------------------------------------

[Test build: ibm]
test_get = ibm
save_stdout_on_success = 1
merge_stdout_stderr = 1

module = Shell
shell_build_command = <<EOT
./configure --enable-static --disable-shared
make -j 32
EOT

#----------------------------------------------------------------------

[Test build: onesided]
test_get = onesided
save_stdout_on_success = 1
merge_stdout_stderr = 1
stderr_save_lines = 100

module = Shell
shell_build_command = <<EOT
./configure
make -j 32
EOT

#----------------------------------------------------------------------

[Test build: mpicxx]
test_get = mpicxx
save_stdout_on_success = 1
merge_stdout_stderr = 1

# Ensure we do not build for the "no C++" install
skip_mpi_install = GNU-No-cxx

module = Shell
shell_build_command = <<EOT
./configure CC=mpicc CXX=mpic++
make -j 16
EOT

#----------------------------------------------------------------------

[Test build: imb-general]
test_get = imb
save_stdout_on_success = 1
merge_stdout_stderr = 1

module = Shell
shell_build_command = <<EOT
cd src
make clean IMB-MPI1 IMB-EXT
EOT

#----------------------------------------------------------------------

[Test build: imb-check]
test_get = imb
save_stdout_on_success = 1
merge_stdout_stderr = 1

module = Shell
shell_build_command = <<EOT
cd src
make clean IMB-MPI1 IMB-EXT CPPFLAGS=-DCHECK
EOT

#----------------------------------------------------------------------

[Test build: netpipe]
test_get = netpipe
save_stdout_on_success = 1
merge_stdout_stderr = 1
stderr_save_lines = 100

module = Shell
shell_build_command = <<EOT
make mpi
EOT

#----------------------------------------------------------------------

[Test build: mpi-test-suite]
test_get = mpi-test-suite
save_stdout_on_success = 1
merge_stdout_stderr = 1
description = HLRS MPI test suite

module = Shell
shell_build_command = <<EOT
./configure CC=mpicc CXX=mpiCC FC=mpifort F77=mpifort --disable-mpi2-io CFLAGS=-g
make
EOT

#----------------------------------------------------------------------

[Test build: java]
test_get = java
save_stdout_on_success = 1
merge_stdout_stderr = 1

# Ensure we only build for the MPI Install named GNU-Java.
mpi_install = GNU-Java

module = Shell
shell_build_command = <<EOT
./configure
make -j 4
EOT

#======================================================================
# Test Run phase
#======================================================================

# This section is not used directly; it is included in others.
[Defaults Test run]
pass = &and(&test_wifexited(), &eq(&test_wexitstatus(), 0))
skipped = &and(&test_wifexited(), &eq(&test_wexitstatus(), 77))

save_stdout_on_pass = 1
merge_stdout_stderr = 1
stdout_save_lines = 100
stderr_save_lines = 100
report_after_n_results = 100

np = &env_max_procs()

#----------------------------------------------------------------------

[Test run: trivial]
include_section = Defaults Test run

test_build = trivial
timeout = &max(10, &test_np())
skipped = 0

specify_module = Simple
simple_first:tests = &find_executables(".")

#----------------------------------------------------------------------

[Test run: intel]
include_section = Defaults Test run

test_build = intel
timeout = &max(30, &multiply(20, &test_np()))
np = &min("60", &env_max_procs())

specify_module = Simple
simple_successful:tests = &find_executables("src")

simple_failures:tests = &find_executables(&prepend("src/", &cat("supposed_to_fail")))
simple_failures:pass = &and(&test_wifexited(), &ne(&test_wexitstatus(), 0))
simple_failures:exclusive = 1
simple_failures:timeout = &env_max_procs()

# These tests sleep for 90 seconds (!) before attempting to process
# any messages
simple_really_slow:tests = src/MPI_Isend_flood_c src/MPI_Send_flood_c
simple_really_slow:pass = &and(&test_wifexited(), &eq(&test_wexitstatus(), 0))
simple_really_slow:exclusive = 1
simple_really_slow:timeout = &sum(180, &multiply(5, &test_np()))

# This test collectively sleeps for 26 seconds *per MCW rank*
simple_coll_slow:tests = src/MPI_collective_overlap_c
simple_coll_slow:pass = &and(&test_wifexited(), &eq(&test_wexitstatus(), 0))
simple_coll_slow:exclusive = 1
simple_coll_slow:timeout = &multiply(35, &test_np())

#------------------------------------------------------------------------

[Test run: ibm]
include_section = Defaults Test run

test_build = ibm
timeout = &max(30, &multiply(10, &test_np()))

specify_module = Simple
simple_first:np = &env_max_procs()
simple_first:tests = &find_executables("collective", "communicator", \
                                       "datatype", "dynamic", "environment", \
                                       "group", "info", "io", "onesided", \
                                       "pt2pt", "random", "topology")

# Tests that are supposed to fail
simple_fail:tests = environment/abort environment/final
simple_fail:pass = &and(&test_wifexited(), &ne(&test_wexitstatus(), 0))
simple_fail:exclusive = 1
simple_fail:timeout = &env_max_procs()

# Spawn tests; need to run very few
simple_spawns:tests  = dynamic/spawn dynamic/spawn_multiple
simple_spawns:np = 3
simple_spawns:pass = &and(&test_wifexited(), &eq(&test_wexitstatus(),0))
simple_spawns:exclusive = 1
simple_spawns:timeout = &multiply(5,&env_max_procs())

# Big loop o' spawns
simple_loop_spawn:tests = dynamic/loop_spawn
simple_loop_spawn:np = 1
simple_loop_spawn:pass = &and(&test_wifexited(), &eq(&test_wexitstatus(),0))
simple_loop_spawn:exclusive = 1
simple_loop_spawn:timeout = 600

# Big loop o' comm splits and whatnot.  It runs for 10 minutes.
simple_loop_comm_split:tests = communicator/comm_split_f
simple_loop_comm_split:np = 1
simple_loop_comm_split:pass = &and(&test_wifexited(), &eq(&test_wexitstatus(),0))
simple_loop_comm_split:exclusive = 1
simple_loop_comm_split:timeout = 660

# THREAD_MULTIPLE test will fail with the openib btl because it
# deactivates itself in the presence of THREAD_MULTIPLE.  So just skip
# it.  loop_child is the target for loop_spawn, so we don't need to
# run it (although it'll safely pass if you run it by itself).
simple_skip:tests = environment/init_thread_multiple dynamic/loop_child
simple_skip:exclusive = 1
simple_skip:do_not_run = 1

#----------------------------------------------------------------------

[Test run: onesided]
include_section = Defaults Test run

test_build = onesided
timeout = &max(30, &multiply(10, &test_np()))

specify_module = Simple
simple_pass:tests = &cat("run_list")

# This test is killing my nodes for some reason
simple_skip:tests = test_dan1
simple_skip:exclusive = 1
simple_skip:do_not_run = 1

#----------------------------------------------------------------------

[Test run: mpicxx]
include_section = Defaults Test run

test_build = mpicxx
timeout = &max(30, &multiply(10, &test_np()))

specify_module = Simple
simple_pass:tests = src/mpi2c++_test src/mpi2c++_dynamics_test

#----------------------------------------------------------------------

[Test run: imb-general]
include_section = Defaults Test run

test_build = imb-general
timeout = &max(2800, &multiply(50, &test_np()))
np = &min("32", &env_max_procs())

argv = -npmin &test_np()

specify_module = Simple
simple_pass:tests = src/IMB-MPI1 src/IMB-EXT

#----------------------------------------------------------------------

[Test run: imb-check]
include_section = Defaults Test run

test_build = imb-check
timeout = &max(2800, &multiply(50, &test_np()))
np = &min("32", &env_max_procs())

argv = -npmin &test_np()

specify_module = Simple
simple_pass:tests = src/IMB-MPI1 src/IMB-EXT

#----------------------------------------------------------------------

[Test run: netpipe-performance]
include_section = Defaults Test run

test_build = netpipe
skipped = 0
timeout = &multiply(&test_np(), 120)
# Ensure to leave this value as "-1", or performance results could be lost!
stdout_save_lines = -1
np = 2

specify_module = Simple
analyze_module = NetPipe
simple_pass:tests = NPmpi

#------------------------------------------------------------------------

# Run the MPI test suite as one big program

[Test run: mpi-test-suite-all]
test_build = mpi-test-suite
pass = &and(&test_wifexited(), &eq(&test_wexitstatus(), 0))
skipped = 0
timeout = &max(600, &multiply(100, &test_np()))
save_stdout_on_pass = 1
merge_stdout_stderr = 1
np = &env_max_procs()

specify_module = Simple
simple_first:tests = mpi_test_suite
# v1.4 requires that we have to ^MPI_TYPE_MIX, ^MPI_SHORT_INT...
# is this still true for master?
# On Rainer's advice, skip the onesided tests
simple_first:argv = -x relaxed -d All,^MPI_TYPE_MIX,^MPI_SHORT_INT -t All,^One-sided

#------------------------------------------------------------------------

# Run the MPI test suite as individual tests

# Currently causing GB of coredumps :-(
# Get those fixed first, then re-enable...
[SKPI Test run: mpi-test-suite-individual]
test_build = mpi-test-suite
pass = &and(&test_wifexited(), &eq(&test_wexitstatus(), 0))
skipped = 0
timeout = &max(600, &multiply(100, &test_np()))
save_stdout_on_pass = 1
merge_stdout_stderr = 1
np = &env_max_procs()

specify_module = Simple
simple_first:tests = mpi_test_suite
# v1.4 requires that we have to ^MPI_TYPE_MIX, ^MPI_SHORT_INT...
# is this still true for master?
# On Rainer's advice, skip the onesided tests
simple_first:argv = -x relaxed -t &step(0, &Cisco::find_hlrs_max_test_num() - 1, 1) -d All,^MPI_TYPE_MIX,^MPI_SHORT_INT -t All,^One-sided

#------------------------------------------------------------------------

# Java test suite

[Test run: java]
test_build = java
pass = &and(&test_wifexited(), &eq(&test_wexitstatus(), 0))
skipped = 0
timeout = 15
save_stdout_on_pass = 1
merge_stdout_stderr = 1
np = &env_max_procs()

specify_module = Simple
simple_first:tests = java
simple_first:argv = &find_java_executables("basic", "ccl_ObjSer", \
                                "collectives", "comm", \
                                "datatype", "dtyp_ObjSer", \
                                "group", "pt2pt", \
                                "pt2pt_ObjSer", "topo")

# Tests that are supposed to fail
simple_fail:tests = java
simple_fail:argv = &java_executable("basic/Abort")
simple_fail:pass = &and(&test_wifexited(), &ne(&test_wexitstatus(), 0))
simple_fail:exclusive = 1
simple_fail:timeout = &env_max_procs()


#======================================================================
# Reporter phase
#======================================================================

[Reporter: IU database]
module = MTTDatabase

mttdatabase_realm = OMPI
mttdatabase_username = cisco
mttdatabase_password = &stringify(&cat("/home/mpiteam/mtt-db-password.txt"))
mttdatabase_platform = cisco-community, usNIC build=&getenv("SLURM_JOB_PARTITION")
mttdatabase_hostname = &if(&eq(&getenv("SLURM_NODELIST"), ""), &env_hosts(), \
                       &getenv("SLURM_NODELIST") . " / SLURM job id " . &getenv("SLURM_JOBID"))
mttdatabase_url = https://mtt.open-mpi.org/submit/
mttdatabase_debug_filename = mttdb_debug_file
mttdatabase_keep_debug_files = 1
mttdatabase_debug_server = 1

#----------------------------------------------------------------------

[SKIP Reporter: send email]
module = Email
email_to = jsquyres@cisco.com
email_subject = MPI test results: $phase / $section

#----------------------------------------------------------------------

# This is a backup for while debugging MTT; it also writes results to
# a local text file
[SKIP Reporter: text file backup]
module = TextFile

textfile_filename = cisco-$phase-$section-$mpi_name-$mpi_version.txt
textfile_separator = >>>>----------------------------------------------------------<<<<
